---
layout: post
title: "RNN & LSTM"
tags:
- deep_learning
---

# 들어가며
이 포스팅에서 다루는 내용은 다음의 블로그 & 강의를 참조하였습니다.
[colah's blog - Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
[NN의 꽃 RNN 이야기](https://www.youtube.com/watch? v=-SHPG_KMUkQ)
[RNN Tutorial](http://aikorea.org/blog/rnn-tutorial-1/)

딥러닝에 대한 약간의 

# RNN
딥러닝은 인간이 생각하는 방법을 모방한 것입니다. 그런데 생각을 해 보면, 우리는 어떤 것을 판단할 떄 매 순간순간마다 생각하지 않습니다.
예를 들면, 이 블로그에 맨 처음 방문한 분은 블로그의 포스팅을 읽으실 건데요. 각각의 포스팅을 읽을 때마다 한 문장 문장을 각기 따로따로 이해하기 보다는, 문장의 맥락을 파악하면서 글을 이해하려고 할 것입니다.

일반적인 뉴럴 네트워크는 문장의 맥락을 판단하기에는 좋지 않은 접근 방법입니다. 입력값은 각각의 가중치를 변경한 이후에는 사라져 버릴 테니까요. 그러나 우리가 하고 싶은 것은 다른 종류의 문제풀이입니다. 예를 들면 '죽인다!' 같은 문장을 입력받았을 때 이 문장이 정말로 살해욕구가 있는 것인지, 아니면 환상적인 무언가를 발견했을 때 내지르는 탄성인지는 앞뒤 맥락을 확인해야만 알 수 있을 것입니다.

이런 문제를 해결할 때는, RNN을 사용합니다.

RNN은 다음과 같이 값을 전달합니다. 많이 보셨을 이미지일 수도 있어요.

![RNN]({{ site.url }}/asset/RNN-rolled.png)

위의 그림은 RNN의 한 구조인데요. A는 x라는 input을 받아 h라는 output을 리턴합니다. 자, 그러면 A는 자기 자신을 재귀적으로 바라보고 있는데요. A 안에서 무슨 일이 발생하는지 알아내야 할 거여요. 실제로 위의 그림을 풀어쓰면, 아래와 같이 나타낼 수 있습니다.

![RNN_2]({{ site.url }}/asset/RNN-unrolled.png)

실제로 A의 내부는 저렇게 되어 있습니다. 실제로 A는 내부에 x를 받아서 h를 리턴하는 신경망을 이어놓은 모양을 하고 있습니다. A에서 생성된 값은 뒤의 신경망에게 전달됩니다. 뒤의 신경망은 또 뒤의 신경망에게 그 값을 전달하게 됩니다. 식으로 보면 다음과 같습니다.

\\[ h_{ t }=f\left( { h }_{ t-1 },\quad { x }_{ t } \right) \\]

즉, 이전의 상태값 \\( { h }_{ t-1 } \\) 과 현재 상태의 인풋값 \\( { x }_{ t } \\) 를 가지고 함수 f(x)에 대입하여 현재 상태를 구하는 것입니다.
그렇다면 여기서 함수 f(x)가 무엇인지 알아야 할 것입니다.

## 함수 f(x)
위에서 보았던 것 처럼, 새로운 상태를 구할 때는 과거의 상태와 현재의 입력값이 필요합니다. 식으로 풀어 쓰면 다음과 같은데요

\\[ h_{ t }=tanh\left( { W }_{ hh }{ h }_{ t-1 }\quad +\quad { W }_{ hx }{ x }_{ t } \right)  \\]

처음 보는 값인 \\( { W }_{ hh } \\) 와 \\( { W }_{ hx } \\) 가 식에 나타났습니다. 위의 두 값은 weight를 나타냅니다.
tanh은 활성함수를 의미합니다. 혹여나 처음 보신다면 시그모이드와 비슷한 것이라고 생각하시면 됩니다.

![RNN_2]({{ site.url }}/asset/Tanh.png)

이렇게 생겼습니다.

그러면 이제 다음 상태를 가져오는 방법을 알게 되었습니다. 다시금 위의 그림으로 돌아가 보면, 현재 상태에서 리턴값을 구해야 하는 경우가 있는데요. 만약 그래야 한다면 다음과 같은 식을 활용하여 리턴값을 구하면 됩니다.

\\[ { y }_{ t }=W_{ hy }{ h }_{ t } \\]

위와 같은 식에 대입하면 리턴값을 구할 수 있습니다.
여기서 주의하실 점은, RNN은 위의 세 weight를 레이어마다 두는 것이 아니라, 이전 상태에서 사용하였던 세 웨이트값을 계속 업데이트 한다는 것입니다. 즉, \\( { W }_{ hh } \\) 와 \\( { W }_{ hx } \\) 그리고 \\( W_{ hy } \\) 를 계속 변경하면서 학습을 진행하게 됩니다.

# RNN의 문제점
앞에서 설명한 RNN을 



