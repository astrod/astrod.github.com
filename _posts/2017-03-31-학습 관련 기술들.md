---
title: "딥러닝 - 학습 관련 기술들"
tags:
- deep_learning
---

# 들어가며
이 포스팅의 내용은 <<밑바닥부터 시작하는 딥러닝>> 6장, 학습 관련 기술들을 정리한 내용입니다.

# 학습 관련 기술들
이전 장에서는(여기 있는 포스팅은 3장, 5장만 있지만) 딥러닝 신경망은 어떻게 구성되고, 경사하강법을 사용하여 가중치는 어떻게 수정하고, 역전파를 이용하여 빠르게 기울기를 구하는 방법을 익혔습니다.

이번 장에서 다룰 내용은 다음과 같습니다

- 가중치 매개변수의 최적값을 탐색하는 최적화 방법
- 가중치 매개변수 초깃값
- 하이퍼파라미터 설정 방법
- 오버피팅의 대응책
	- 가중치 감수
	- 드롭아웃

## 매개변수 갱신
신경망 학습의 목적은 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것입니다. 이는 곧 최적 매개변수를 찾는 문제이고,이러한 문제를 푸는 것을 최적화라고 합니다. 매개변수 공간은 매우 넓고 복잡하여, 최적 매개변수를 찾는 것은 쉽지 않은 문제입니다.

이전 장에서 최적 매개변수를 찾기 위해서 매개변수의 기울기를 이용했습니다. 손실 함수를 구하고, 손실 함수의 값이 0에 가까워지게 매개변수를 지속적으로 수정해 나갔습니다. 이를 확률적 경사 하강법(SGD)라고 합니다.

그러나 문제에 따라서는,SGD보다 똑똑한 방법이 있습니다. 이 장에서는 그런 방법을 이야기하고자 합니다.

### 확률적 경사 하강법(SGD)
먼저 고전적인 방법인 SGD를 다시 한번 확인해 보겠습니다. 수식으로는 다음과 같이 쓸 수 있습니다.

\\[ W\xleftarrow {  } W-\eta \frac { \sigma L }{ \sigma W } \\]


여기에서 W는 갱신할 가중치 매개변수이고, \\( \frac { \sigma L }{ \sigma W } \\) 는 W에 대한 손실 함수의 기울기입니다.

\\( \eta \\)는 여기서 학습률을 의미하는데, 학습률은 실제로는 0.01이나 0.001과 같은 값을 미리 정하여 사용합니다.
이 SGD를 파이썬 클래스로 구현하면 다음과 같습니다.

~~~python
class SDG:
	def __init__(self, lr=0.01):
		self.lr = lr # 학습률을 의미한다.
	
	def update(self, param, grads):
		for key in params.keys():
			params[key] -= self.lr * grads[key] # 가중치 매개변수 -= 학습률 * 기울기
~~~

### SGD의 단점
SDG는 구현은 쉽지만, 문제에 따라 비효율적일 때가 있습니다.
다음 함수의 최솟값을 구하는 문제를 생각해보겠습니다.

\\[ f\left( x,y \right) =\frac { 1 }{ 20 } { x }^{ 2 }+{ y }^{ 2 } \\]

이 함수는 밥그릇을 x축 방향으로 늘린 듯한 모습입니다. 이 함수의 기울기를 그려 보면, y축 방향은 가파른 데 비해 x축 방향은 작게 그려집니다. 

또한 위의 식이 최솟값이 되는 장소는 (x,y) = (0, 0)이지만, 실제로 기울기 대부분은 (0,0)을 가리키지 않습니다. 즉, 경사하강법을 사용하면 이상한 곳에 도달할 확률이 높습니다.

이를 정리하면 다음과 같습니다.

> SGD는 비등방성(방향에 따라 성질, 즉 여기서는 기울기가 달라지는 함수)에서는 탐색 경로가 비효율적이다.

이제부터 위의 문제점을 개선해주는 모멘텀, AdaGrad, Adam이라는 세 방법을 소개하겠습니다. 이들은 모두 SGD를 대체하는 방법입니다.

### 모멘텀
이 기법은 수식으로는 다음과 같이 나타낼 수 있습니다.

\\[ \upsilon \quad \xleftarrow {  } \alpha \upsilon -\eta \frac { \sigma L }{ \sigma W } \\]

위에서 갱신한 v값을 가지고 아래 W를 갱신합니다.

W <- W + v

W는 갱신할 가중치 매개변수, \\( \frac { \sigma L }{ \sigma W } \\) 는 W에 대한 손실함수의 기울기, \\( \eta \\) 는 학습률입니다. v라는 변수가 새로 나오는데, 이는 물리에서 말하는 속도에 해당합니다.

모멘텀이 추가되면, 다음 두 가지 속성을 갖게 됩니다.

1. 공이 그릇의 바닥을 구르는 듯한 움직임으로 손실함수값이 갱신됩니다.
2. av값은 물체가 아무런 힘을 받지 않을 때, 서서히 하강하게 만듭니다. (a값은 0.9 등의 상수로 설정합니다)

소스코드는 다음과 같습니다.

~~~python
class Momentum:
	def __init__(self, lr=0.01, momentum=0.9):
		self.lr = lr
		self.momentum = momentum
		self.v = None # 물체의 속도, 초기화 때는 아무 값도 담지 않음
		
	def update(self, params, grads):
		if self.v is None:
			self.v = {} 
			for key, val in paras.items():
				self.v[key] =np.zeros_like(val) # update 시에 None 이면 매개변수와 같은 구조의 데이터로, 0을 채워서 생성
			
		for key in params.keys():
			# self.v[key]는 이전 update때 움직였던 가중치값(항상 마이너스)
			self.v[key] = self.momentum * self.v[key] - self.lr*grads[key]
			params[key] += self.v[key] 
			# self.v[key]는 음수로 시작. 이전에 많이 이동했다면(-가 크다면), 다음 update에도 많이 움직인다. 이전에 조금 이동했다면 다음 update에는 더 조금 움직이게 된다. 즉, 갑작스럽게 방향전환이 되지 않고 한 방향으로 이동하게 된다.
			
~~~

이를 이용하여 위의 문제를 풀면, 공이 그릇의 바닥을 구르듯 움직입니다. 

- x축의 힘은 아주 작지만 방향이 변하지 않습니다. 
- y축의 힘은 크지만 위아래로 번갈아 움직여 속도는 안정적이지 않습니다.

### AdaGrad
신경망 학습에서는 학습률이 중요합니다. 수식으로는 \\( \eta \\)입니다. 이 학습률을 정하는 기술로 학습률 감소가 있습니다. 이는 학습을 진행하면서 학습률을 점차 줄여가는 방법입니다.

학습률을 낮추는 방법으로는 전체의 학습률 값을 일괄적으로 낮추는 것이 있는데요. 이를 발전시킨 것이 AdaGrad 입니다. AdaGrad는 각각의 매개변수의 맞춤형 값을 만들어줍니다.

수식으로는 다음과 같습니다.

\\[ h\quad \xleftarrow {  } h+\frac { \sigma L }{ \sigma W } \circledcirc \frac { \sigma L }{ \sigma W } \\]

\\[ W\xleftarrow {  } W-\eta \frac { 1 }{ \sqrt { h }  } \frac { \sigma L }{ \sigma W } \\]

- W : 갱신할 가중치 매개변수
- \\( \frac { \sigma L }{ \sigma W } \\) : W에 대한 손실 함수의 기울기
- h : 기존 기울기 값을 계속 제곱하여 더해준 값

이런 후에 매개변수를 갱신할 때, \\( \frac { 1 }{ \sqrt { h }  } \\)를 곱해 학습률을 조정합니다. 즉, 많이 갱신된 함수는 학습률 \\( \eta \\)가 낮아지게 되는 것입니다.

또한, AdaGrad는 과거의 기울기를 계속 제곱하여 더해가기 때문에, 학습을 진행할수록 갱신 강도가 약해집니다. 어느 순간 갱신량이 0이 되어서 전혀 갱신되지 않게 됩니다. 이를 개선한 기법으로 RMSProp이 있는데, 이는 먼 과거의 기울기는 서서히 잊고 새로운 기울기 정보를 크게 반영하는 학습 방법입니다.

코드는 다음과 같습니다.

~~~python
class AdaGrad:
	def __init__(self, lr=0.01):
		self.lr = lr
		self.h = None
		
	def update(self, paras, grads):
		if self.h is None
			self.h = {}
			for key, val in params.items():
				self.h[key] = np.zeros_like(val)
		
		for key in params.keys():
			self.h[key] += grads[key] * grads[key]
			paras[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)			# 계속 제곱하다 보면 self.h[key]값이 0이 될 수 있기 때문에, 이를 막기 위해 1e-7을 더해준다.
~~~

이를 통해 최적화 문제를 풀면, 맨 처음에는 y축 방향으로 크게 움직이지만, 학습 강도가 크기 때문에, 갱신 강도도 큰 폭으로 줄어듭니다. 따라서 y축으로 갱신 강도가 약해져 지그재그 움직임이 줄어듭니다.

### Adam
모멘텀은 공이 구르는 듯한 물리 법칙을 따르는 움직임을 보였습니다.
AdaGrad는 매개변수의 원소마다 적응적으로 갱신 강도를 조정했습니다. 이 두 기법을 융합한 것이 Adam입니다.

이 글에서는 직관적으로만 설명하겠습니다. 필요하다면 해당 논문 확인 부탁 드립니다.

Adam또한 갱신 과정도 그릇에서 공이 구르는 듯 움직입니다.

### 어떤 갱신 방법을 이용할 것인가?
풀어야 할 문제에 따라 어떤 갱신 기법을 사용할지 달라집니다. 각각의 장단이 있어서 잘 푸는 문제와 서툰 문제가 있습니다.

지금도 많은 연구에서 SGD를 사용하고 있고, 모멘텀과 AdaGrad도 시도해볼 만한 가치가 있습니다. 또한 요즘에는 많은 분들이 Adam에 만족해하고 계십니다. 각자의 상황을 고려하여 여러 가지 시도해보고, 가장 효율적인 것을 사용하면 될 거 같습니다.

## 가중치의 초깃값
신경망 학습에서 특히 중요한 것이 가중치의 초깃값입니다. 권장하는 가중치 초깃값과 실험을 통해 실제로 어떻게 초깃값이 신경망 학습에 영향을 미치는지 확인할 예정입니다.

### 초깃값을 0으로 하면?

