---
title: "딥러닝 - 학습 관련 기술들"
tags:
- deep_learning
---

# 들어가며
이 포스팅의 내용은 <<밑바닥부터 시작하는 딥러닝>> 6장, 학습 관련 기술들을 정리한 내용입니다.

# 학습 관련 기술들
이전 장에서는(여기 있는 포스팅은 3장, 5장만 있지만) 딥러닝 신경망은 어떻게 구성되고, 경사하강법을 사용하여 가중치는 어떻게 수정하고, 역전파를 이용하여 빠르게 기울기를 구하는 방법을 익혔습니다.

이번 장에서 다룰 내용은 다음과 같습니다

- 가중치 매개변수의 최적값을 탐색하는 최적화 방법
- 가중치 매개변수 초깃값
- 하이퍼파라미터 설정 방법
- 오버피팅의 대응책
	- 가중치 감수
	- 드롭아웃

## 매개변수 갱신
신경망 학습의 목적은 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것입니다. 이는 곧 최적 매개변수를 찾는 문제이고,이러한 문제를 푸는 것을 최적화라고 합니다. 매개변수 공간은 매우 넓고 복잡하여, 최적 매개변수를 찾는 것은 쉽지 않은 문제입니다.

이전 장에서 최적 매개변수를 찾기 위해서 매개변수의 기울기를 이용했습니다. 손실 함수를 구하고, 손실 함수의 값이 0에 가까워지게 매개변수를 지속적으로 수정해 나갔습니다. 이를 확률적 경사 하강법(SGD)라고 합니다.

그러나 문제에 따라서는,SGD보다 똑똑한 방법이 있습니다. 이 장에서는 그런 방법을 이야기하고자 합니다.

### 확률적 경사 하강법(SGD)
먼저 고전적인 방법인 SGD를 다시 한번 확인해 보겠습니다. 수식으로는 다음과 같이 쓸 수 있습니다.

\[ W\xleftarrow {  } W-\eta \frac { \sigma L }{ \sigma W } /]


여기에서 W는 갱신할 가중치 매개변수이고, \\( \frac { \sigma L }{ \sigma W } \\) 는 W에 대한 손실 함수의 기울기입니다.