---
title: "딥러닝 - 학습 관련 기술들"
tags:
- deep_learning
---

# 들어가며
이 포스팅의 내용은 <<밑바닥부터 시작하는 딥러닝>> 6장, 학습 관련 기술들을 정리한 내용입니다.

# 학습 관련 기술들
이전 장에서는(여기 있는 포스팅은 3장, 5장만 있지만) 딥러닝 신경망은 어떻게 구성되고, 경사하강법을 사용하여 가중치는 어떻게 수정하고, 역전파를 이용하여 빠르게 기울기를 구하는 방법을 익혔습니다.

이번 장에서 다룰 내용은 다음과 같습니다

- 가중치 매개변수의 최적값을 탐색하는 최적화 방법
- 가중치 매개변수 초깃값
- 하이퍼파라미터 설정 방법
- 오버피팅의 대응책
	- 가중치 감수
	- 드롭아웃

## 매개변수 갱신
신경망 학습의 목적은 손실 함수의 값을 가능한 한 낮추는 매개변수를 찾는 것입니다. 이는 곧 최적 매개변수를 찾는 문제이고,이러한 문제를 푸는 것을 최적화라고 합니다. 매개변수 공간은 매우 넓고 복잡하여, 최적 매개변수를 찾는 것은 쉽지 않은 문제입니다.

이전 장에서 최적 매개변수를 찾기 위해서 매개변수의 기울기를 이용했습니다. 손실 함수를 구하고, 손실 함수의 값이 0에 가까워지게 매개변수를 지속적으로 수정해 나갔습니다. 이를 확률적 경사 하강법(SGD)라고 합니다.

그러나 문제에 따라서는,SGD보다 똑똑한 방법이 있습니다. 이 장에서는 그런 방법을 이야기하고자 합니다.

### 확률적 경사 하강법(SGD)
먼저 고전적인 방법인 SGD를 다시 한번 확인해 보겠습니다. 수식으로는 다음과 같이 쓸 수 있습니다.

\\[ W\xleftarrow {  } W-\eta \frac { \sigma L }{ \sigma W } \\]


여기에서 W는 갱신할 가중치 매개변수이고, \\( \frac { \sigma L }{ \sigma W } \\) 는 W에 대한 손실 함수의 기울기입니다.

\\( \eta \\)는 여기서 학습률을 의미하는데, 학습률은 실제로는 0.01이나 0.001과 같은 값을 미리 정하여 사용합니다.
이 SGD를 파이썬 클래스로 구현하면 다음과 같습니다.

~~~python
class SDG:
	def __init__(self, lr=0.01):
		self.lr = lr # 학습률을 의미한다.
	
	def update(self, param, grads):
		for key in params.keys():
			params[key] -= self.lr * grads[key] # 가중치 매개변수 -= 학습률 * 기울기
~~~

### SGD의 단점
SDG는 구현은 쉽지만, 문제에 따라 비효율적일 때가 있습니다.
다음 함수의 최솟값을 구하는 문제를 생각해보겠습니다.

\\[ f\left( x,y \right) =\frac { 1 }{ 20 } { x }^{ 2 }+{ y }^{ 2 } \\]

이 함수는 밥그릇을 x축 방향으로 늘린 듯한 모습입니다. 이 함수의 기울기를 그려 보면, y축 방향은 가파른 데 비해 x축 방향은 작게 그려집니다. 

또한 위의 식이 최솟값이 되는 장소는 (x,y) = (0, 0)이지만, 실제로 기울기 대부분은 (0,0)을 가리키지 않습니다. 즉, 경사하강법을 사용하면 이상한 곳에 도달할 확률이 높습니다.

이를 정리하면 다음과 같습니다.

> SGD는 비등방성(방향에 따라 성질, 즉 여기서는 기울기가 달라지는 함수)에서는 탐색 경로가 비효율적이다.

이제부터 위의 문제점을 개선해주는 모멘텀, AdaGrad, Adam이라는 세 방법을 소개하겠습니다. 이들은 모두 SGD를 대체하는 방법입니다.

### 모멘텀
이 기법은 수식으로는 다음과 같이 나타낼 수 있습니다.

\\[ \upsilon \quad \xleftarrow {  } \alpha \upsilon \quad -\quad \eta \frac { \sigma L }{ \sigma W } \\]

위에서 갱신한 v값을 가지고 아래 W를 갱신합니다.

W <- W + v

W는 갱신할 가중치 매개변수, \\( \frac { \sigma L }{ \sigma W } \\) 는 W에 대한 손실함수의 기울기, \\( \eta \\) 는 학습률입니다. v라는 변수가 새로 나오는데, 이는 물리에서 말하는 속도에 해당합니다.

모멘텀이 추가되면, 다음 두 가지 속성을 갖게 됩니다.

1. 공이 그릇의 바닥을 구르는 듯한 움직임으로 손실함수값이 갱신됩니다.
2. av값은 물체가 아무런 힘을 받지 않을 때, 서서히 하강하게 만듭니다. (a값은 0.9 등의 상수로 설정합니다)

소스코드는 다음과 같습니다.

~~~python
class Momentum:
	def __init__(self, lr=0.01, momentum=0.9):
		self.lr = lr
		self.momentum = momentum
		self.v = None # 물체의 속도, 초기화 때는 아무 값도 담지 않음
		
	def update(self, params, grads):
		if self.v is None:
			self.v = {} 
			for key, val in paras.items():
				self.v[key] =np.zeros_like(val) # update 시에 None 이면 매개변수와 같은 구조의 데이터로, 0을 채워서 생성
			
		for key in params.keys():
			# self.v[key]는 이전 update때 움직였던 가중치값(항상 마이너스)
			self.v[key] = self.momentum * self.v[key] - self.lr*grads[key]
			params[key] += self.v[key] 
			# self.v[key]는 음수로 시작. 이전에 많이 이동했다면(-가 크다면), 다음 update에도 많이 움직인다. 이전에 조금 이동했다면 다음 update에는 더 조금 움직이게 된다. 즉, 갑작스럽게 방향전환이 되지 않고 한 방향으로 이동하게 된다.
			
~~~

이를 이용하여 위의 문제를 풀면, 공이 그릇의 바닥을 구르듯 움직입니다. 

- x축의 힘은 아주 작지만 방향이 변하지 않습니다. 
- y축의 힘은 크지만 위아래로 번갈아 움직여 속도는 안정적이지 않습니다.

### AdaGrad
신경망 학습에서는 학습률이 중요합니다. 수식으로는 \\( \eta \\)입니다. 이 학습률을 정하는 기술로 학습률 감소가 있습니다. 이는 학습을 진행하면서 학습률을 점차 줄여가는 방법입니다.

학습률을 낮추는 방법으로는 전체의 학습률 값을 일괄적으로 낮추는 것이 있는데요. 이를 발전시킨 것이 AdaGrad 입니다. AdaGrad는 각각의 매개변수의 맞춤형 값을 만들어줍니다.

수식으로는 다음과 같습니다.


